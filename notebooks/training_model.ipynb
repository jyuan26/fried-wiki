{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.jooble.com:8080\n",
      "Collecting sentence_transformers==0.3.9\n",
      "  Downloading sentence-transformers-0.3.9.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (4.47.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (1.7.1)\n",
      "Requirement already satisfied: numpy in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (0.23.1)\n",
      "Requirement already satisfied: scipy in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (1.5.0)\n",
      "Requirement already satisfied: nltk in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from sentence_transformers==0.3.9) (3.5)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp38-cp38-macosx_10_6_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.15.1)\n",
      "Requirement already satisfied: filelock in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.0.12)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp38-cp38-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 32.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2020.6.8)\n",
      "Requirement already satisfied: requests in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.24.0)\n",
      "Requirement already satisfied: packaging in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (20.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers==0.3.9) (3.7.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence_transformers==0.3.9) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence_transformers==0.3.9) (0.16.0)\n",
      "Requirement already satisfied: click in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from nltk->sentence_transformers==0.3.9) (7.1.2)\n",
      "Requirement already satisfied: six>=1.9 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ntr/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.4.7)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-py3-none-any.whl size=101032 sha256=b0deddd8a8b5ae5a1aa405b7f654a853ed0992baa8e4dbf533fef1f5a515f9db\n",
      "  Stored in directory: /Users/ntr/Library/Caches/pip/wheels/a2/37/5a/dc15355b55ec28d07d59ef19f8240fde65bb654d05233205da\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.95\n",
      "    Uninstalling sentencepiece-0.1.95:\n",
      "      Successfully uninstalled sentencepiece-0.1.95\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.2\n",
      "    Uninstalling transformers-4.2.2:\n",
      "      Successfully uninstalled transformers-4.2.2\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 0.4.1.2\n",
      "    Uninstalling sentence-transformers-0.4.1.2:\n",
      "      Successfully uninstalled sentence-transformers-0.4.1.2\n",
      "Successfully installed sentence-transformers-0.3.9 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers==0.3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, LabelAccuracyEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "def _read_tsv(input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            if sys.version_info[0] == 2:\n",
    "                line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "    \n",
    "def _create_examples_snli(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[7]\n",
    "        text_b = line[8]\n",
    "        label = line[-1]\n",
    "        examples.append([guid, text_a, text_b, label])\n",
    "    return examples\n",
    "\n",
    "def _create_examples_mnli(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[8]\n",
    "        text_b = line[9]\n",
    "        label = line[-1]\n",
    "        examples.append([guid, text_a, text_b, label])\n",
    "    return examples\n",
    "\n",
    "# train_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/train.tsv'), 'train_s')\n",
    "# dev_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/dev.tsv'), 'dev_s')\n",
    "# test_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/test.tsv'), 'test_s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-27 21:39:12 - Read AllNLI train dataset\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "# train_nli_samples = []\n",
    "# dev_nli_samples = []\n",
    "# test_nli_samples = []\n",
    "\n",
    "# for row in tqdm(train_snli):\n",
    "#     label_id = label2int[row[3]]\n",
    "#     train_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "# for row in tqdm(dev_snli):\n",
    "#     label_id = label2int[row[3]]\n",
    "#     dev_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\n",
    "# for row in tqdm(test_snli):\n",
    "#     label_id = label2int[row[3]]\n",
    "#     test_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-27 21:39:17 - Use pytorch device: cpu\n",
      "2021-02-27 21:39:17 - Softmax loss: #Vectors concatenated: 3\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 64\n",
    "\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = models.BERT(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# train_data_nli = SentencesDataset(train_nli_samples, model=model)\n",
    "# train_dataloader_nli = DataLoader(train_data_nli, shuffle=True, batch_size=batch_size)\n",
    "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
    "\n",
    "\n",
    "# dev_data_nli = SentencesDataset(dev_nli_samples, model=model)\n",
    "# dev_dataloader_nli = DataLoader(dev_data_nli, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# test_data_nli = SentencesDataset(test_nli_samples, model=model)\n",
    "# test_dataloader_nli = DataLoader(test_data_nli, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2304, out_features=3, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_nli.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0357b0e41677413e8bb464b2c08f7989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927ab49e1ef8480cb48877933c2675e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/8584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Read the dataset and initial model\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_save_path = 'output/snli_'+model_name+'-full_iteration_'\n",
    "\n",
    "# num_epochs = 1\n",
    "# warmup_steps = math.ceil(len(train_data_nli) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "# logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "# train_objectives = [(train_dataloader_nli, train_loss_nli)]\n",
    "\n",
    "# validation_performance = []\n",
    "# test_performance = []\n",
    "\n",
    "# test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "# dev_evaluator = LabelAccuracyEvaluator(dev_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n",
    "\n",
    "# validation_performance.append(model.evaluate(dev_evaluator))\n",
    "# test_performance.append(model.evaluate(test_evaluator))\n",
    "# print(f'Iteration - {0} ...')\n",
    "# print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "# print(f'Test performance - {test_performance[-1]} ...')\n",
    "\n",
    "for i in range(1):\n",
    "    model.fit(train_objectives=train_objectives, output_path=model_save_path+str(i+1))\n",
    "#     validation_performance.append(model.evaluate(dev_evaluator))\n",
    "#     test_performance.append(model.evaluate(test_evaluator))\n",
    "#     print(f'Iteration - {i+1} ...')\n",
    "#     print(f'Validation performance - {validation_performance[-1]} ...')\n",
    "#     print(f'Test performance - {test_performance[-1]} ...')\n",
    "#     model.save(\"kaggle/working/\" + model_save_path+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bert_model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: bert_model_trained/\t(in=0) (out=0) (stored 0%)\n",
      "  adding: bert_model_trained/config.json\t(in=28) (out=28) (stored 0%)\n",
      "  adding: bert_model_trained/1_Pooling/\t(in=0) (out=0) (stored 0%)\n",
      "  adding: bert_model_trained/1_Pooling/config.json\t(in=190) (out=100) (deflated 47%)\n",
      "  adding: bert_model_trained/modules.json\t(in=228) (out=111) (deflated 51%)\n",
      "  adding: bert_model_trained/0_BERT/\t(in=0) (out=0) (stored 0%)\n",
      "  adding: bert_model_trained/0_BERT/config.json\t(in=502) (out=272) (deflated 46%)\n",
      "  adding: bert_model_trained/0_BERT/pytorch_model.bin .........................................\t(in=438015479) (out=405261278) (deflated 7%)\n",
      "  adding: bert_model_trained/0_BERT/vocab.txt \t(in=231508) (out=109776) (deflated 53%)\n",
      "  adding: bert_model_trained/0_BERT/special_tokens_map.json\t(in=112) (out=67) (deflated 40%)\n",
      "  adding: bert_model_trained/0_BERT/sentence_bert_config.json\t(in=27) (out=27) (stored 0%)\n",
      "  adding: bert_model_trained/0_BERT/tokenizer_config.json\t(in=300) (out=180) (deflated 40%)\n",
      "total bytes=438248374, compressed=405371839 -> 8% savings\n"
     ]
    }
   ],
   "source": [
    "!zip -vr results_2.zip bert_model_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"Your file path\"> Download File </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(train_loss_nli.classifier.cpu(), 'classifier_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://sbert.net/models/bert-base-uncased.zip. Response 404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2618ae6aec52468e9915365484fe4b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41af33e00f345949887866b1e40919b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadcbec581d247b59bd14be04bf21e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [i[1] for i in train_snli] + [i[2] for i in train_snli] \\\n",
    "        + [i[1] for i in test_snli] + [i[2] for i in test_snli] \\\n",
    "        + [i[1] for i in dev_snli] + [i[2] for i in dev_snli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6578715a985d445b8816586796cee6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = list(set(texts))\n",
    "embeds = model.encode(texts, convert_to_numpy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_cpu = [i.cpu() for i in embeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {t:e for t,e in zip(texts, embeds_cpu)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('texts_cache.pickle', 'wb') as handle:\n",
    "    pickle.dump(cache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b7ff24a59664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"
     ]
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-5b387e5c1cb3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-5b387e5c1cb3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from IPython.display import FileLink -> FileLink(r'texts_cache.pickle')\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'texts_cache.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_path = \"../modules/models/bert_model_trained_2/\"\n",
    "classification_model_path = \"../modules/models/classifier_model_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-23 19:26:25 - Load pretrained SentenceTransformer: ../modules/models/bert_model_trained_2/\n",
      "2021-02-23 19:26:25 - Load SentenceTransformer from folder: ../modules/models/bert_model_trained_2/\n",
      "2021-02-23 19:26:27 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "bert_model = SentenceTransformer(bert_model_path)\n",
    "classification_model = torch.load(classification_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2304, out_features=3, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
