{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/general-language-understanding-evaluation/STS/STS-B/test.tsv\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/train.tsv\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/readme.txt\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/LICENSE.txt\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/original/sts-train.tsv\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/original/sts-dev.tsv\n/kaggle/input/general-language-understanding-evaluation/STS/STS-B/original/sts-test.tsv\n/kaggle/input/general-language-understanding-evaluation/QNLI/QNLI/test.tsv\n/kaggle/input/general-language-understanding-evaluation/QNLI/QNLI/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/QNLI/QNLI/train.tsv\n/kaggle/input/general-language-understanding-evaluation/RTE/RTE/test.tsv\n/kaggle/input/general-language-understanding-evaluation/RTE/RTE/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/RTE/RTE/train.tsv\n/kaggle/input/general-language-understanding-evaluation/QQP/QQP/test.tsv\n/kaggle/input/general-language-understanding-evaluation/QQP/QQP/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/QQP/QQP/train.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/diagnostic.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/diagnostic-full.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/test_mismatched.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/dev_matched.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/train.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/README.txt\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/test_matched.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/dev_mismatched.tsv\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_dev_mismatched.jsonl\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_dev_matched.txt\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_dev_mismatched.txt\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_train.txt\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_train.jsonl\n/kaggle/input/general-language-understanding-evaluation/MNLI/MNLI/original/multinli_1.0_dev_matched.jsonl\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/test.tsv\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/train.tsv\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/README.txt\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_train.txt\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_dev.txt\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_test.jsonl\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_test.txt\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_dev.jsonl\n/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/original/snli_1.0_train.jsonl\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/test.tsv\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/train.tsv\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/datasetSentences.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/STree.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/README.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/original_rt_snippets.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/SOStr.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/dictionary.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/sentiment_labels.txt\n/kaggle/input/general-language-understanding-evaluation/SST/SST-2/original/datasetSplit.txt\n/kaggle/input/general-language-understanding-evaluation/WNLI/WNLI/test.tsv\n/kaggle/input/general-language-understanding-evaluation/WNLI/WNLI/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/WNLI/WNLI/train.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/test.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/dev.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/train.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/tokenized/out_of_domain_dev.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/tokenized/in_domain_train.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/tokenized/in_domain_dev.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/raw/out_of_domain_dev.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/raw/in_domain_train.tsv\n/kaggle/input/general-language-understanding-evaluation/CoLA/CoLA/original/raw/in_domain_dev.tsv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sentence_transformers==0.3.9","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting sentence_transformers==0.3.9\n  Downloading sentence-transformers-0.3.9.tar.gz (64 kB)\n\u001b[K     |████████████████████████████████| 64 kB 558 kB/s eta 0:00:01\n\u001b[?25hCollecting transformers<3.6.0,>=3.1.0\n  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n\u001b[K     |████████████████████████████████| 1.3 MB 1.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (4.55.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (1.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (0.23.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (1.4.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence_transformers==0.3.9) (3.2.4)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence_transformers==0.3.9) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence_transformers==0.3.9) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence_transformers==0.3.9) (0.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (20.8)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2020.11.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.0.12)\nCollecting tokenizers==0.9.3\n  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n\u001b[K     |████████████████████████████████| 2.9 MB 4.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.14.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (0.0.43)\nCollecting sentencepiece==0.1.91\n  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 7.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence_transformers==0.3.9) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (1.26.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (1.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence_transformers==0.3.9) (7.1.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers==0.3.9) (2.1.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-py3-none-any.whl size=101031 sha256=88fa79e64a90f6d8cfbf972361c49070132742153564eb244985e817a9ebf992\n  Stored in directory: /root/.cache/pip/wheels/b0/57/6e/72164eb7d28256df352bcce26174d9133d191c232006fb13d2\nSuccessfully built sentence-transformers\nInstalling collected packages: tokenizers, sentencepiece, transformers, sentence-transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.9.4\n    Uninstalling tokenizers-0.9.4:\n      Successfully uninstalled tokenizers-0.9.4\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.1.95\n    Uninstalling sentencepiece-0.1.95:\n      Successfully uninstalled sentencepiece-0.1.95\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.0.1\n    Uninstalling transformers-4.0.1:\n      Successfully uninstalled transformers-4.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 1.3.0 requires transformers<4.1,>=4.0, but you have transformers 3.5.1 which is incompatible.\u001b[0m\nSuccessfully installed sentence-transformers-0.3.9 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport math\nfrom sentence_transformers import models, losses\nfrom sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, LabelAccuracyEvaluator\nfrom sentence_transformers.readers import *\nimport logging\nfrom datetime import datetime\nimport gzip\nimport csv\nimport os\n\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport csv\nimport sys\n\n#### Just some code to print debug information to stdout\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\n#### /print debug information to stdout\n\ndef _read_tsv(input_file, quotechar=None):\n    \"\"\"Reads a tab separated value file.\"\"\"\n    with open(input_file, \"r\", encoding='utf-8') as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n        lines = []\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, 'utf-8') for cell in line)\n            lines.append(line)\n        return lines\n    \ndef _create_examples_snli(lines, set_type):\n    \"\"\"Creates examples for the training and dev sets.\"\"\"\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (set_type, line[0])\n        text_a = line[7]\n        text_b = line[8]\n        label = line[-1]\n        examples.append([guid, text_a, text_b, label])\n    return examples\n\ndef _create_examples_mnli(lines, set_type):\n    \"\"\"Creates examples for the training and dev sets.\"\"\"\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (set_type, line[0])\n        text_a = line[8]\n        text_b = line[9]\n        label = line[-1]\n        examples.append([guid, text_a, text_b, label])\n    return examples\n\ntrain_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/train.tsv'), 'train_s')\ndev_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/dev.tsv'), 'dev_s')\ntest_snli = _create_examples_snli(_read_tsv('/kaggle/input/general-language-understanding-evaluation/SNLI/SNLI/test.tsv'), 'test_s')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the dataset to a DataLoader ready for training\nlogging.info(\"Read AllNLI train dataset\")\nlabel2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\ntrain_nli_samples = []\ndev_nli_samples = []\ntest_nli_samples = []\n\nfor row in tqdm(train_snli):\n    label_id = label2int[row[3]]\n    train_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\nfor row in tqdm(dev_snli):\n    label_id = label2int[row[3]]\n    dev_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))\nfor row in tqdm(test_snli):\n    label_id = label2int[row[3]]\n    test_nli_samples.append(InputExample(guid = row[0], texts=[row[1], row[2]], label=label_id))","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/549367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039fc3c5260c4b1f8ada416938e543dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9842 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ebb2d42800e4e96b19f081dabaa69fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9824 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f8a10f4e46415c918345df85cfb367"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the dataset\nmodel_name = 'bert-base-uncased'\nbatch_size = 64\n\n# Use BERT for mapping tokens to embeddings\nword_embedding_model = models.BERT(model_name)\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\ntrain_data_nli = SentencesDataset(train_nli_samples, model=model)\ntrain_dataloader_nli = DataLoader(train_data_nli, shuffle=True, batch_size=batch_size)\ntrain_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n\n\ndev_data_nli = SentencesDataset(dev_nli_samples, model=model)\ndev_dataloader_nli = DataLoader(dev_data_nli, shuffle=True, batch_size=batch_size)\n\ntest_data_nli = SentencesDataset(test_nli_samples, model=model)\ntest_dataloader_nli = DataLoader(test_data_nli, shuffle=True, batch_size=batch_size)","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c7f82b0ce44e838d9ebeacae1947f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dac1de7f51a4eb48725502dc01872bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7d863a6297c46409e3d8cad0afe8d29"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Read the dataset and initial model\n# model_name = 'bert-base-uncased'\n# model_save_path = 'output/snli_'+model_name+'-full_iteration_'\n\n# num_epochs = 1\n# warmup_steps = math.ceil(len(train_data_nli) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n# logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n# train_objectives = [(train_dataloader_nli, train_loss_nli)]\n\n# validation_performance = []\n# test_performance = []\n\n# test_evaluator = LabelAccuracyEvaluator(test_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n# dev_evaluator = LabelAccuracyEvaluator(dev_dataloader_nli, name = 'nli_test', softmax_model = train_loss_nli)\n\n# validation_performance.append(model.evaluate(dev_evaluator))\n# test_performance.append(model.evaluate(test_evaluator))\n# print(f'Iteration - {0} ...')\n# print(f'Validation performance - {validation_performance[-1]} ...')\n# print(f'Test performance - {test_performance[-1]} ...')\n\nfor i in range(1):\n    model.fit(train_objectives=train_objectives, output_path=model_save_path+str(i+1))\n#     validation_performance.append(model.evaluate(dev_evaluator))\n#     test_performance.append(model.evaluate(test_evaluator))\n#     print(f'Iteration - {i+1} ...')\n#     print(f'Validation performance - {validation_performance[-1]} ...')\n#     print(f'Test performance - {test_performance[-1]} ...')\n#     model.save(\"kaggle/working/\" + model_save_path+str(i+1))","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0357b0e41677413e8bb464b2c08f7989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/8584 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927ab49e1ef8480cb48877933c2675e4"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"bert_model_trained\")","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -vr results_2.zip bert_model_trained/","execution_count":17,"outputs":[{"output_type":"stream","text":"  adding: bert_model_trained/\t(in=0) (out=0) (stored 0%)\n  adding: bert_model_trained/config.json\t(in=28) (out=28) (stored 0%)\n  adding: bert_model_trained/1_Pooling/\t(in=0) (out=0) (stored 0%)\n  adding: bert_model_trained/1_Pooling/config.json\t(in=190) (out=100) (deflated 47%)\n  adding: bert_model_trained/modules.json\t(in=228) (out=111) (deflated 51%)\n  adding: bert_model_trained/0_BERT/\t(in=0) (out=0) (stored 0%)\n  adding: bert_model_trained/0_BERT/config.json\t(in=502) (out=272) (deflated 46%)\n  adding: bert_model_trained/0_BERT/pytorch_model.bin .........................................\t(in=438015479) (out=405261278) (deflated 7%)\n  adding: bert_model_trained/0_BERT/vocab.txt \t(in=231508) (out=109776) (deflated 53%)\n  adding: bert_model_trained/0_BERT/special_tokens_map.json\t(in=112) (out=67) (deflated 40%)\n  adding: bert_model_trained/0_BERT/sentence_bert_config.json\t(in=27) (out=27) (stored 0%)\n  adding: bert_model_trained/0_BERT/tokenizer_config.json\t(in=300) (out=180) (deflated 40%)\ntotal bytes=438248374, compressed=405371839 -> 8% savings\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"Your file path\"> Download File </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.save(train_loss_nli.classifier.cpu(), 'classifier_model_2')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('bert-base-uncased')","execution_count":6,"outputs":[{"output_type":"stream","text":"Exception when trying to download https://sbert.net/models/bert-base-uncased.zip. Response 404\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2618ae6aec52468e9915365484fe4b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41af33e00f345949887866b1e40919b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aadcbec581d247b59bd14be04bf21e81"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = [i[1] for i in train_snli] + [i[2] for i in train_snli] \\\n        + [i[1] for i in test_snli] + [i[2] for i in test_snli] \\\n        + [i[1] for i in dev_snli] + [i[2] for i in dev_snli]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = list(set(texts))\nembeds = model.encode(texts, convert_to_numpy=False)","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/20360 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6578715a985d445b8816586796cee6f1"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeds_cpu = [i.cpu() for i in embeds]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cache = {t:e for t,e in zip(texts, embeds_cpu)}","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open('texts_cache.pickle', 'wb') as handle:\n    pickle.dump(cache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"done\")","execution_count":27,"outputs":[{"output_type":"stream","text":"done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cache","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'cache' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b7ff24a59664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'texts_cache.pickle')","execution_count":28,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-28-5b387e5c1cb3>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-5b387e5c1cb3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from IPython.display import FileLink -> FileLink(r'texts_cache.pickle')\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}